# RAG ì„œë²„ (ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì§€ì›)

Milvus ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ í”„ë¡œë•ì…˜ ë ˆë”” RAG(Retrieval-Augmented Generation) ì„œë²„ì…ë‹ˆë‹¤. ë‹¤ì¤‘ LLM ì§€ì›ê³¼ í¬ê´„ì ì¸ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸš€ ì£¼ìš” ê¸°ëŠ¥

### ë‹¤ì¤‘ LLM ì§€ì›
- **OpenAI**: GPT-3.5, GPT-4 ëª¨ë¸
- **Anthropic**: Claude ëª¨ë¸
- **Google**: Gemini ëª¨ë¸  
- **Ollama**: ë¡œì»¬ ëª¨ë¸ ì§€ì›
- í™•ì¥ ê°€ëŠ¥í•œ LLM í”„ë¡œë°”ì´ë” í”„ë ˆì„ì›Œí¬

### ë‹¤ì¤‘ ë°ì´í„°ë² ì´ìŠ¤ ì§€ì›
- **ë°ì´í„°ë² ì´ìŠ¤**: PostgreSQL, MySQL, MariaDB, Oracle, SQL Server
- **ê³ ê¸‰ ì—°ê²° ê´€ë¦¬**: ì—°ê²° í’€ë§, í—¬ìŠ¤ ëª¨ë‹ˆí„°ë§, ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
- **íŒŒì¼ ì†ŒìŠ¤**: TXT, PDF, Word, Excel, Markdown
- ìë™ ìŠ¤í‚¤ë§ˆ ê°ì§€ ë° ì¸íŠ¸ë¡œìŠ¤í™ì…˜

### ì„¸ë°€í•œ ì ‘ê·¼ ì œì–´ (FGAC)
- Milvus í–‰ ìˆ˜ì¤€ RBAC í†µí•©
- ì‚¬ìš©ì/ê·¸ë£¹ ê¸°ë°˜ ê¶Œí•œ ê´€ë¦¬
- ë¦¬ì†ŒìŠ¤ ìˆ˜ì¤€ ì ‘ê·¼ ì œì–´
- JWT ê¸°ë°˜ ì¸ì¦

### ì´ì¤‘ RAG ìš´ì˜ ëª¨ë“œ
- **ë‹¨ì¼ LLM ëª¨ë“œ**: ë¹ ë¥¸ ë‹¨ì¼ ëª¨ë¸ ì‘ë‹µ
- **ë‹¤ì¤‘ LLM ëª¨ë“œ**: í•©ì˜ ê¸°ë°˜ ë‹¤ì¤‘ ëª¨ë¸ ì‘ë‹µ

### í”„ë¡œë•ì…˜ ë ˆë”” ë°ì´í„°ë² ì´ìŠ¤ ë ˆì´ì–´
- **Milvus** ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í†µí•©
- í¬ê´„ì ì¸ í—¬ìŠ¤ ëª¨ë‹ˆí„°ë§
- ì§€ëŠ¥í˜• ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
- ë‚´ê²°í•¨ì„±ì„ ìœ„í•œ ì„œí‚· ë¸Œë ˆì´ì»¤ íŒ¨í„´

## ğŸ“‹ ìš”êµ¬ì‚¬í•­

- **Python**: 3.11+
- **Milvus**: 2.3.0+
- **ë©”ëª¨ë¦¬**: ìµœì†Œ 8GB (ê¶Œì¥ 16GB+)
- **ìŠ¤í† ë¦¬ì§€**: ë°ì´í„°ë² ì´ìŠ¤ ë° ë²¡í„° ì €ì¥ ê³µê°„

## ğŸ› ï¸ ì„¤ì¹˜ ë°©ë²•

### 1. ë ˆí¬ì§€í† ë¦¬ í´ë¡ 

```bash
git clone <repository-url>
cd novelbot_RAG_server

# uvë¡œ ì˜ì¡´ì„± ì„¤ì¹˜
uv sync

# ê°œë°œ ì˜ì¡´ì„± ì„¤ì¹˜
uv sync --group dev
```

### 2. í™˜ê²½ ì„¤ì •

```bash
# í™˜ê²½ ë³€ìˆ˜ í…œí”Œë¦¿ ë³µì‚¬
cp .env.example .env

# .env íŒŒì¼ì—ì„œ ì„¤ì • ìˆ˜ì •
vim .env

# í•„ìˆ˜ ì„¤ì •:
# - ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´ (DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD)
# - Milvus ì—°ê²° ì •ë³´ (MILVUS_HOST, MILVUS_PORT)
# - LLM ë° ì„ë² ë”© í”„ë¡œë°”ì´ë” ì„¤ì •
# - API í‚¤ ì„¤ì • (ì‚¬ìš©í•˜ëŠ” í”„ë¡œë°”ì´ë”ì— ë”°ë¼)
```

### 3. ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰

```bash
# ì„œë²„ ì‹œì‘
uv run main.py

# ë˜ëŠ” CLI ì‚¬ìš©
uv run rag-cli serve --reload
```

## âš™ï¸ ì„¤ì •

ì„¤ì •ì€ `.env` íŒŒì¼ì„ í†µí•´ í™˜ê²½ ë³€ìˆ˜ë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤. ì„¤ì • ì˜ˆì‹œëŠ” `.env.example` íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

### ì„ë² ë”© ì„¤ì •

```bash
# Ollama ë¡œì»¬ ì„ë² ë”© (ë¬´ë£Œ) [ê¶Œì¥]
EMBEDDING_PROVIDER=ollama
EMBEDDING_MODEL=jeffh/intfloat-multilingual-e5-large-instruct:f32
EMBEDDING_API_KEY=

# OpenAI ì„ë² ë”© (ìœ ë£Œ)
# EMBEDDING_PROVIDER=openai
# EMBEDDING_MODEL=text-embedding-3-large
# EMBEDDING_API_KEY=your-openai-api-key

# Google ì„ë² ë”© (ìœ ë£Œ)
# EMBEDDING_PROVIDER=google
# EMBEDDING_MODEL=text-embedding-004
# EMBEDDING_API_KEY=your-google-api-key
```

### ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •

```bash
# MySQL/MariaDB (ê¸°ë³¸ê°’)
DB_HOST=localhost
DB_PORT=3306
DB_NAME=novelbot
DB_USER=root
DB_PASSWORD=password

# PostgreSQL ì‚¬ìš© ì‹œ
# DB_HOST=localhost
# DB_PORT=5432
# DB_NAME=ragdb
# DB_USER=postgres
# DB_PASSWORD=password
```

### LLM ì„¤ì •

```bash
# Ollama ë¡œì»¬ LLM (ë¬´ë£Œ) [ê¶Œì¥]
LLM_PROVIDER=ollama
LLM_MODEL=gemma3:27b-it-q8_0
LLM_API_KEY=

# OpenAI (ìœ ë£Œ)
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-3.5-turbo
# LLM_API_KEY=your-openai-api-key

# Anthropic Claude (ìœ ë£Œ)
# LLM_PROVIDER=anthropic
# LLM_MODEL=claude-3-5-sonnet-latest
# LLM_API_KEY=your-anthropic-api-key

# Google Gemini (ìœ ë£Œ)
# LLM_PROVIDER=google
# LLM_MODEL=gemini-2.0-flash-001
# LLM_API_KEY=your-google-api-key
```

### Milvus ì„¤ì •

```bash
MILVUS_HOST=localhost
MILVUS_PORT=19530
# ë¡œì»¬ Milvusì—ì„œëŠ” ì¸ì¦ ì—†ì´ ì‚¬ìš© ê°€ëŠ¥
MILVUS_USER=
MILVUS_PASSWORD=
```

### API ì„œë²„ ì„¤ì •

```bash
API_HOST=0.0.0.0
API_PORT=8000
SECRET_KEY=your-secret-key-here
```


## ğŸ“¡ API ì‚¬ìš©ë²•

### ê¸°ë³¸ LLM ì¿¼ë¦¬

```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "messages": [
      {"role": "user", "content": "ë¨¸ì‹ ëŸ¬ë‹ì´ ë¬´ì—‡ì¸ê°€ìš”?"}
    ],
    "model": "gpt-4",
    "temperature": 0.7,
    "max_tokens": 1000
  }'
```

### ìŠ¤íŠ¸ë¦¬ë° LLM ì‘ë‹µ

```bash
curl -X POST "http://localhost:8000/chat/stream" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "messages": [
      {"role": "user", "content": "ì–‘ì ì»´í“¨íŒ…ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”"}
    ],
    "model": "claude-3-5-sonnet-latest",
    "temperature": 0.7,
    "stream": true
  }'
```

### ë‹¤ì¤‘ LLM ë¡œë“œ ë°¸ëŸ°ì‹±

```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "messages": [
      {"role": "user", "content": "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ” ì–´ë–»ê²Œ ë ê¹Œìš”?"}
    ],
    "load_balancing": "health_based",
    "temperature": 0.8,
    "max_tokens": 1500
  }'
```

### RAG ì¿¼ë¦¬ (ë²¡í„° ê²€ìƒ‰ + LLM)

```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "query": "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì˜ ì¢…ë¥˜ì™€ íŠ¹ì§•",
    "mode": "rag",
    "k": 5,
    "llm_provider": "openai",
    "model": "gpt-4"
  }'
```

### ì„ë² ë”© ìƒì„±

```bash
curl -X POST "http://localhost:8000/embeddings" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "input": ["ë¨¸ì‹ ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ê°€?", "ë”¥ëŸ¬ë‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì°¨ì´ì "],
    "model": "text-embedding-3-large",
    "dimensions": 1024,
    "normalize": true
  }'
```

### Ollama ë¡œì»¬ ì„ë² ë”© ìƒì„±

```bash
curl -X POST "http://localhost:8000/embeddings" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "input": ["ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ  ì„¤ëª…"],
    "provider": "ollama",
    "model": "nomic-embed-text",
    "normalize": true
  }'
```

### ë‹¤ì¤‘ ì„ë² ë”© í”„ë¡œë°”ì´ë” ì‚¬ìš©

```bash
curl -X POST "http://localhost:8000/embeddings" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "input": ["ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì˜ ë°œì „"],
    "load_balancing": "cost_optimized",
    "dimensions": 512,
    "normalize": true
  }'
```

### ë‹¨ì¼ LLM ì‘ë‹µ ìƒì„± (ê³ ì† ëª¨ë“œ)

```bash
curl -X POST "http://localhost:8000/generate/single" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "query": "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    "mode": "fast",
    "context": "AI ê¸°ìˆ  ê´€ë ¨ êµìœ¡ ìë£Œ",
    "model": "gpt-4",
    "temperature": 0.7,
    "max_tokens": 1000,
    "response_format": "markdown"
  }'
```

### ë‹¤ì¤‘ LLM ì•™ìƒë¸” ì‘ë‹µ (ê³ í’ˆì§ˆ ëª¨ë“œ)

```bash
curl -X POST "http://localhost:8000/generate/ensemble" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "query": "ì¸ê³µì§€ëŠ¥ì˜ ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­ì— ëŒ€í•´ ë¶„ì„í•´ì£¼ì„¸ìš”",
    "mode": "high_quality", 
    "ensemble_size": 3,
    "consensus_threshold": 0.7,
    "enable_parallel_generation": true,
    "evaluation_metrics": ["relevance", "accuracy", "completeness"],
    "output_format": "structured",
    "custom_instructions": "ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ê· í˜•ì¡íŒ ë¶„ì„ ì œê³µ"
  }'
```

### RAG ê¸°ë°˜ ê³ ê¸‰ ì‘ë‹µ ìƒì„±

```bash
curl -X POST "http://localhost:8000/generate/rag" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "query": "ìµœì‹  íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì˜ ë°œì „ ë™í–¥",
    "retrieval_config": {
      "k": 10,
      "search_strategy": "hybrid",
      "include_metadata": true,
      "enable_reranking": true
    },
    "generation_config": {
      "mode": "ensemble",
      "ensemble_size": 2,
      "prompt_strategy": "contextual",
      "enable_post_processing": true,
      "response_format": "markdown"
    },
    "quality_filters": {
      "min_relevance": 0.8,
      "min_completeness": 0.7,
      "enable_fact_checking": true
    }
  }'
```

### RDB ë°ì´í„° ì¶”ì¶œ

```bash
curl -X POST "http://localhost:8000/extract/rdb" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "database_config": {
      "host": "localhost",
      "port": 5432,
      "database": "mydb",
      "user": "postgres",
      "password": "password",
      "database_type": "postgresql"
    },
    "extraction_config": {
      "mode": "incremental",
      "batch_size": 1000,
      "include_tables": ["users", "orders", "products"],
      "incremental_column": "updated_at",
      "validate_data": true
    }
  }'
```

### íŒŒì¼ ì‹œìŠ¤í…œ ë°°ì¹˜ ì²˜ë¦¬

```bash
curl -X POST "http://localhost:8000/extract/filesystem" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "directory": "/path/to/documents",
    "batch_config": {
      "max_workers": 8,
      "batch_size": 100,
      "memory_limit_mb": 2048,
      "enable_adaptive_batching": true,
      "processing_strategy": "priority_based"
    },
    "file_patterns": ["*.pdf", "*.docx", "*.txt"],
    "exclude_patterns": ["*.tmp"],
    "progress_callback_url": "http://localhost:8000/progress"
  }'
```

## ğŸ’» í”„ë¡œê·¸ë˜ë° ì‚¬ìš©ë²•

### ë‹¤ì¤‘ LLM ë§¤ë‹ˆì € ì‚¬ìš©

```python
import asyncio
from src.llm import (
    LLMManager, LLMProvider, LLMConfig, LLMRequest, 
    LLMMessage, LLMRole, ProviderConfig, LoadBalancingStrategy
)

async def main():
    # í”„ë¡œë°”ì´ë” ì„¤ì •
    provider_configs = [
        ProviderConfig(
            provider=LLMProvider.OPENAI,
            config=LLMConfig(
                provider=LLMProvider.OPENAI,
                api_key="your-openai-key",
                model="gpt-4",
                temperature=0.7
            )
        ),
        ProviderConfig(
            provider=LLMProvider.CLAUDE,
            config=LLMConfig(
                provider=LLMProvider.CLAUDE,
                api_key="your-anthropic-key",
                model="claude-3-5-sonnet-latest",
                temperature=0.7
            )
        )
    ]
    
    # LLM ë§¤ë‹ˆì € ì´ˆê¸°í™”
    llm_manager = LLMManager(provider_configs)
    llm_manager.set_load_balancing_strategy(LoadBalancingStrategy.HEALTH_BASED)
    
    # ë©”ì‹œì§€ ìƒì„±
    messages = [
        LLMMessage(role=LLMRole.USER, content="ì•ˆë…•í•˜ì„¸ìš”, ë¨¸ì‹ ëŸ¬ë‹ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”")
    ]
    
    request = LLMRequest(
        messages=messages,
        model="gpt-4",
        temperature=0.7,
        max_tokens=1000
    )
    
    # ì‘ë‹µ ìƒì„±
    response = await llm_manager.generate_async(request)
    print(f"ì‘ë‹µ: {response.content}")
    print(f"ì‚¬ìš©ëœ í”„ë¡œë°”ì´ë”: {response.metadata.get('provider')}")
    
    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
    async for chunk in llm_manager.generate_stream_async(request):
        if chunk.content:
            print(chunk.content, end='', flush=True)

asyncio.run(main())
```

### ë‹¨ì¼ ë° ì•™ìƒë¸” ì‘ë‹µ ìƒì„± ì‹œìŠ¤í…œ ì‚¬ìš©

```python
import asyncio
from src.response_generation import (
    SingleLLMGenerator, EnsembleLLMGenerator, ResponseRequest,
    ResponseGeneratorConfig, ResponseMode, EvaluationMetric,
    PromptStrategy, ContextInjectionMode, OutputFormat
)
from src.llm import LLMManager
from src.rag.context_retriever import DocumentContext

async def response_generation_example():
    # ì‘ë‹µ ìƒì„± ì„¤ì •
    config = ResponseGeneratorConfig(
        single_timeout=30.0,
        ensemble_timeout=90.0,
        ensemble_size=3,
        consensus_threshold=0.7,
        min_quality_score=0.6,
        enable_parallel_generation=True,
        enable_post_processing=True,
        enable_quality_filtering=True,
        evaluation_metrics=[
            EvaluationMetric.RELEVANCE,
            EvaluationMetric.ACCURACY,
            EvaluationMetric.COMPLETENESS
        ]
    )
    
    # LLM ë§¤ë‹ˆì € ì´ˆê¸°í™” (ì´ì „ ì˜ˆì œì—ì„œ ì„¤ì •)
    llm_manager = LLMManager(provider_configs)
    
    # ë‹¨ì¼ LLM ìƒì„±ê¸°
    single_generator = SingleLLMGenerator(llm_manager, config)
    
    # ì•™ìƒë¸” LLM ìƒì„±ê¸°
    ensemble_generator = EnsembleLLMGenerator(llm_manager, config)
    
    # ìš”ì²­ ìƒì„±
    request = ResponseRequest(
        query="ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ ì°¨ì´ì ì„ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        context="AI ê¸°ìˆ  êµìœ¡ ê³¼ì •ì˜ ê¸°ì´ˆ í•™ìŠµ ìë£Œ",
        model="gpt-4",
        temperature=0.7,
        max_tokens=1500,
        system_prompt="ë‹¹ì‹ ì€ AI ê¸°ìˆ  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
        custom_instructions="ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.",
        response_format="markdown"
    )
    
    # ë‹¨ì¼ LLM ì‘ë‹µ ìƒì„± (ê³ ì† ëª¨ë“œ)
    print("=== ë‹¨ì¼ LLM ì‘ë‹µ ìƒì„± ===")
    single_result = await single_generator.generate_response_async(request)
    
    print(f"ì‘ë‹µ: {single_result.response}")
    print(f"ì‚¬ìš©ëœ í”„ë¡œë°”ì´ë”: {single_result.provider_used}")
    print(f"ì‘ë‹µ ì‹œê°„: {single_result.response_time:.2f}ì´ˆ")
    print(f"í’ˆì§ˆ ì ìˆ˜: {single_result.overall_quality_score:.3f}")
    
    # ì•™ìƒë¸” LLM ì‘ë‹µ ìƒì„± (ê³ í’ˆì§ˆ ëª¨ë“œ)
    print("\n=== ì•™ìƒë¸” LLM ì‘ë‹µ ìƒì„± ===")
    ensemble_result = await ensemble_generator.generate_response_async(request)
    
    print(f"ìµœì  ì‘ë‹µ: {ensemble_result.best_response.response}")
    print(f"ì‚¬ìš©ëœ í”„ë¡œë°”ì´ë”ë“¤: {ensemble_result.providers_used}")
    print(f"ì»¨ì„¼ì„œìŠ¤ ì ìˆ˜: {ensemble_result.consensus_score:.3f}")
    print(f"ì„ íƒ ë°©ë²•: {ensemble_result.selection_method}")
    print(f"ì´ ì‘ë‹µ ìˆ˜: {len(ensemble_result.all_responses)}")
    print(f"ì•™ìƒë¸” ì²˜ë¦¬ ì‹œê°„: {ensemble_result.ensemble_time:.2f}ì´ˆ")
    
    # ëª¨ë“  ì‘ë‹µ ë¹„êµ
    print("\n=== ëª¨ë“  ì•™ìƒë¸” ì‘ë‹µ ë¹„êµ ===")
    for i, response in enumerate(ensemble_result.all_responses, 1):
        print(f"ì‘ë‹µ {i} (í”„ë¡œë°”ì´ë”: {response.provider_used}):")
        print(f"  í’ˆì§ˆ ì ìˆ˜: {response.overall_quality_score:.3f}")
        print(f"  ì‘ë‹µ ê¸¸ì´: {len(response.response)} ë¬¸ì")
        
        # ê°œë³„ í’ˆì§ˆ ë©”íŠ¸ë¦­
        for score in response.quality_scores:
            print(f"  {score.metric.value}: {score.score:.3f} (ì‹ ë¢°ë„: {score.confidence:.3f})")
        print()

asyncio.run(response_generation_example())
```

### ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì‚¬ìš©

```python
from src.response_generation import (
    PromptEngineer, PromptStrategy, ContextInjectionMode,
    PromptTemplate, ContextRelevanceFilter
)

async def prompt_engineering_example():
    # í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ ì´ˆê¸°í™”
    prompt_engineer = PromptEngineer(config)
    
    # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¶”ê°€
    custom_template = PromptTemplate(
        name="detailed_analysis",
        template="""ë‹¹ì‹ ì€ {domain} ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        
ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

{context}

ì§ˆë¬¸: {query}

ë‹µë³€ ìš”êµ¬ì‚¬í•­:
- êµ¬ì²´ì ì¸ ì˜ˆì‹œ í¬í•¨
- ë‹¨ê³„ë³„ ì„¤ëª…
- ì‹¤ìš©ì  ê´€ì ì—ì„œì˜ ë¶„ì„
- {response_format} í˜•ì‹ìœ¼ë¡œ ì‘ì„±

{custom_instructions}""",
        required_variables=["query", "domain"],
        optional_variables=["context", "response_format", "custom_instructions"]
    )
    
    prompt_engineer.add_custom_template(custom_template)
    
    # ì»¨í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ìš”ì²­
    request_with_context = ResponseRequest(
        query="íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì˜ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì‘ë™ ì›ë¦¬",
        retrieval_result=RetrievalResult(
            contexts=[
                DocumentContext(
                    id="doc1",
                    content="ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì€ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ëª¨ë“  ìœ„ì¹˜ì— ëŒ€í•´ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤...",
                    similarity_score=0.95,
                    source_info={"title": "Attention Is All You Need", "type": "paper"}
                ),
                DocumentContext(
                    id="doc2", 
                    content="ì…€í”„ ì–´í…ì…˜ì€ ì¿¼ë¦¬, í‚¤, ë°¸ë¥˜ ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ë©ë‹ˆë‹¤...",
                    similarity_score=0.88,
                    source_info={"title": "íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡° ë¶„ì„", "type": "tutorial"}
                )
            ]
        ),
        user_context={"domain": "ë”¥ëŸ¬ë‹", "level": "intermediate"},
        response_format="markdown",
        custom_instructions="ìˆ˜ì‹ê³¼ ê·¸ë¦¼ìœ¼ë¡œ ì„¤ëª…"
    )
    
    # ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ ì „ëµ í…ŒìŠ¤íŠ¸
    strategies = [
        (PromptStrategy.STRUCTURED, ContextInjectionMode.STRUCTURED_SECTIONS),
        (PromptStrategy.CONTEXTUAL, ContextInjectionMode.ADAPTIVE),
        (PromptStrategy.HIERARCHICAL, ContextInjectionMode.INTERLEAVED)
    ]
    
    for strategy, injection_mode in strategies:
        print(f"\n=== {strategy.value} ì „ëµ, {injection_mode.value} ì£¼ì… ëª¨ë“œ ===")
        
        engineered_prompt, metadata = prompt_engineer.engineer_prompt(
            request_with_context, strategy, injection_mode
        )
        
        print(f"ì‚¬ìš©ëœ í…œí”Œë¦¿: {metadata['template_used']}")
        print(f"ì»¨í…ìŠ¤íŠ¸ ìˆ˜: {metadata['contexts_used']}")
        print(f"ì›ë³¸ ê¸¸ì´: {metadata['original_length']}")
        print(f"ìµœì í™” ê¸¸ì´: {metadata['optimized_length']}")
        print(f"ì••ì¶• ì ìš©: {metadata['compression_applied']}")
        print(f"í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {engineered_prompt[:200]}...")

asyncio.run(prompt_engineering_example())
```

### ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ë° í›„ì²˜ë¦¬ ì‚¬ìš©

```python
from src.response_generation import (
    ResponseEvaluator, ResponseProcessor, EvaluationWeights,
    QualityThresholds, OutputFormat, FormattingRules, ValidationRules
)

async def evaluation_and_processing_example():
    # ì‘ë‹µ í‰ê°€ê¸° ì„¤ì •
    evaluator = ResponseEvaluator(config)
    
    # ì»¤ìŠ¤í…€ í‰ê°€ ê°€ì¤‘ì¹˜ ì„¤ì •
    custom_weights = EvaluationWeights(
        relevance=0.35,
        accuracy=0.30,
        completeness=0.20,
        clarity=0.10,
        coherence=0.05
    )
    evaluator.set_evaluation_weights(custom_weights)
    
    # ì»¤ìŠ¤í…€ í’ˆì§ˆ ì„ê³„ê°’ ì„¤ì •
    custom_thresholds = QualityThresholds(
        min_relevance=0.7,
        min_accuracy=0.6,
        min_completeness=0.5,
        min_overall=0.65
    )
    evaluator.set_quality_thresholds(custom_thresholds)
    
    # ì‘ë‹µ í›„ì²˜ë¦¬ê¸° ì„¤ì •
    processor = ResponseProcessor(config)
    
    # ì»¤ìŠ¤í…€ í¬ë§·íŒ… ê·œì¹™
    formatting_rules = FormattingRules(
        max_line_length=100,
        enable_auto_paragraphs=True,
        enable_auto_lists=True,
        enable_auto_headers=True,
        fix_grammar=True,
        normalize_punctuation=True
    )
    processor.set_formatting_rules(formatting_rules)
    
    # ê²€ì¦ ê·œì¹™
    validation_rules = ValidationRules(
        min_length=50,
        max_length=5000,
        check_completeness=True,
        check_coherence=True,
        remove_hallucinations=True
    )
    processor.set_validation_rules(validation_rules)
    
    # ìƒ˜í”Œ ì‘ë‹µë“¤ ìƒì„± (ì‹¤ì œë¡œëŠ” LLMì—ì„œ ìƒì„±)
    sample_responses = [
        ResponseResult(
            response="ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë”¥ëŸ¬ë‹ì€ ì‹ ê²½ë§ì„ ì‚¬ìš©í•œ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼ë¡œ...",
            llm_response=LLMResponse(content="...", metadata={"provider": "openai"})
        ),
        ResponseResult(
            response="ì¸ê³µì§€ëŠ¥ì˜ í•˜ìœ„ ë¶„ì•¼ì¸ ë¨¸ì‹ ëŸ¬ë‹ì€ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ë°ì´í„°ì—ì„œ ìë™ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. ë”¥ëŸ¬ë‹ì€...",
            llm_response=LLMResponse(content="...", metadata={"provider": "claude"})
        )
    ]
    
    # ê° ì‘ë‹µ í‰ê°€
    evaluated_responses = []
    for response in sample_responses:
        evaluated_response = evaluator.evaluate_response(response, request)
        evaluated_responses.append(evaluated_response)
        
        print(f"ì‘ë‹µ í‰ê°€ ê²°ê³¼:")
        print(f"  ì „ì²´ í’ˆì§ˆ ì ìˆ˜: {evaluated_response.overall_quality_score:.3f}")
        print(f"  ì‹ ë¢°ë„ ì ìˆ˜: {evaluated_response.confidence_score:.3f}")
        
        for score in evaluated_response.quality_scores:
            print(f"  {score.metric.value}: {score.score:.3f} ({score.explanation})")
    
    # ìµœì  ì‘ë‹µ ì„ íƒ
    best_response, consensus, method = evaluator.select_best_response(
        evaluated_responses, request
    )
    
    print(f"\nìµœì  ì‘ë‹µ ì„ íƒë¨ (ë°©ë²•: {method}, ì»¨ì„¼ì„œìŠ¤: {consensus:.3f})")
    
    # ë‹¤ì–‘í•œ í¬ë§·ìœ¼ë¡œ í›„ì²˜ë¦¬
    output_formats = [
        OutputFormat.PLAIN_TEXT,
        OutputFormat.MARKDOWN,
        OutputFormat.HTML,
        OutputFormat.JSON,
        OutputFormat.STRUCTURED
    ]
    
    for format_type in output_formats:
        processed_response = processor.process_response(
            best_response, request, format_type
        )
        
        print(f"\n=== {format_type.value} í¬ë§· ===")
        print(f"ì²˜ë¦¬ëœ ì‘ë‹µ: {processed_response.response[:200]}...")
        print(f"ì²˜ë¦¬ ë‹¨ê³„: {processed_response.processing_steps}")

asyncio.run(evaluation_and_processing_example())
```

### ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ ì‹œìŠ¤í…œ ì‚¬ìš©

```python
from src.response_generation import (
    ErrorHandler, ErrorPolicy, ErrorSeverity, ErrorCategory,
    RetryStrategy, CircuitBreakerConfig, TimeoutConfig
)

async def error_handling_example():
    # ì—ëŸ¬ í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”
    error_handler = ErrorHandler(config)
    
    # ì»¤ìŠ¤í…€ ì„œí‚· ë¸Œë ˆì´ì»¤ ì„¤ì •
    cb_config = CircuitBreakerConfig(
        failure_threshold=3,
        recovery_timeout=60.0,
        success_threshold=2,
        enable_half_open=True
    )
    
    # ì»¤ìŠ¤í…€ íƒ€ì„ì•„ì›ƒ ì„¤ì •
    timeout_config = TimeoutConfig(
        default_timeout=30.0,
        slow_timeout=60.0,
        fast_timeout=15.0,
        ensemble_timeout=120.0,
        enable_adaptive_timeout=True
    )
    
    # ì»¤ìŠ¤í…€ ì—ëŸ¬ ì •ì±… ì¶”ê°€
    custom_policy = ErrorPolicy(
        error_types=[ConnectionError, TimeoutError],
        severity=ErrorSeverity.HIGH,
        category=ErrorCategory.NETWORK,
        retry_strategy=RetryStrategy.EXPONENTIAL,
        max_retries=3,
        base_delay=2.0,
        should_fallback=True,
        should_circuit_break=True
    )
    
    # ì‹œë®¬ë ˆì´ì…˜: ì—ëŸ¬ ì²˜ë¦¬
    try:
        # íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ í•¨ìˆ˜ ì‹¤í–‰
        result = await error_handler.with_timeout_async(
            some_llm_function,
            timeout=30.0,
            request_data
        )
        
        # ì„œí‚· ë¸Œë ˆì´ì»¤ì™€ í•¨ê»˜ ì‹¤í–‰
        result = await error_handler.with_circuit_breaker_async(
            some_provider_function,
            provider="openai",
            request_data
        )
        
    except Exception as e:
        # ì—ëŸ¬ ì²˜ë¦¬ ê²°ì •
        context = {"provider": "openai", "model": "gpt-4", "operation": "generate"}
        decision = error_handler.handle_error(e, context, retry_count=0)
        
        print(f"ì—ëŸ¬ ì²˜ë¦¬ ê²°ì •: {decision['action']}")
        print(f"ì¬ì‹œë„ ì§€ì—°: {decision['delay']}ì´ˆ")
        print(f"í´ë°± í•„ìš”: {decision['should_fallback']}")
        print(f"ì„œí‚· ë¸Œë ˆì´ì»¤ íŠ¸ë¦¬ê±°: {decision['should_circuit_break']}")
        
        # ì¬ì‹œë„ ì—¬ë¶€ í™•ì¸
        should_retry, delay = error_handler.should_retry(e, retry_count=0, context=context)
        
        if should_retry:
            print(f"ì¬ì‹œë„ ì˜ˆì • ({delay}ì´ˆ í›„)")
            await asyncio.sleep(delay)
            # ì¬ì‹œë„ ë¡œì§...
        else:
            print("ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ, í´ë°± ë˜ëŠ” ì‹¤íŒ¨ ì²˜ë¦¬")
    
    # ì—ëŸ¬ í†µê³„ í™•ì¸
    stats = error_handler.get_error_statistics()
    print(f"\nì—ëŸ¬ í†µê³„:")
    print(f"ì´ ì—ëŸ¬ ìˆ˜: {stats['total_errors']}")
    print(f"ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜: {stats['category_breakdown']}")
    print(f"ì‹¬ê°ë„ë³„ ë¶„ë¥˜: {stats['severity_breakdown']}")
    print(f"í”„ë¡œë°”ì´ë”ë³„ ë¶„ë¥˜: {stats['provider_breakdown']}")
    print(f"ì„œí‚· ë¸Œë ˆì´ì»¤ ìƒíƒœ: {stats['circuit_breaker_states']}")

asyncio.run(error_handling_example())
```

### ê°œë³„ í”„ë¡œë°”ì´ë” ì‚¬ìš©

```python
from src.llm import OpenAIProvider, LLMConfig, LLMProvider

# OpenAI í”„ë¡œë°”ì´ë” ì´ˆê¸°í™”
config = LLMConfig(
    provider=LLMProvider.OPENAI,
    api_key="your-openai-key",
    model="gpt-4",
    temperature=0.7
)

provider = OpenAIProvider(config)

# ì‘ë‹µ ìƒì„±
response = await provider.generate_async(request)
print(response.content)
```

### ì„ë² ë”© ë§¤ë‹ˆì € ì‚¬ìš©

```python
import asyncio
from src.embedding import (
    EmbeddingManager, EmbeddingProvider, EmbeddingConfig, EmbeddingRequest,
    EmbeddingProviderConfig, EmbeddingLoadBalancingStrategy
)

async def main():
    # í”„ë¡œë°”ì´ë” ì„¤ì •
    provider_configs = [
        EmbeddingProviderConfig(
            provider=EmbeddingProvider.OPENAI,
            config=EmbeddingConfig(
                provider=EmbeddingProvider.OPENAI,
                api_key="your-openai-key",
                model="text-embedding-3-large",
                dimensions=1024
            ),
            priority=1,
            cost_per_1m_tokens=0.13
        ),
        EmbeddingProviderConfig(
            provider=EmbeddingProvider.GOOGLE,
            config=EmbeddingConfig(
                provider=EmbeddingProvider.GOOGLE,
                api_key="your-google-key",
                model="text-embedding-004",
                dimensions=768
            ),
            priority=2,
            cost_per_1m_tokens=0.025
        ),
        EmbeddingProviderConfig(
            provider=EmbeddingProvider.OLLAMA,
            config=EmbeddingConfig(
                provider=EmbeddingProvider.OLLAMA,
                model="nomic-embed-text",
                base_url="http://localhost:11434",
                dimensions=768
            ),
            priority=3,
            cost_per_1m_tokens=0.0  # ë¡œì»¬ ëª¨ë¸ì€ ë¬´ë£Œ
        )
    ]
    
    # ì„ë² ë”© ë§¤ë‹ˆì € ì´ˆê¸°í™”
    embedding_manager = EmbeddingManager(provider_configs, enable_cache=True)
    embedding_manager.set_load_balancing_strategy(EmbeddingLoadBalancingStrategy.COST_OPTIMIZED)
    
    # ì„ë² ë”© ìƒì„±
    request = EmbeddingRequest(
        input=["ë¨¸ì‹ ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ê°€?"],
        model="text-embedding-3-large",
        dimensions=512,
        normalize=True
    )
    
    response = await embedding_manager.generate_embeddings_async(request)
    print(f"ì„ë² ë”© ì°¨ì›: {response.dimensions}")
    print(f"ì‚¬ìš©ëœ í”„ë¡œë°”ì´ë”: {response.metadata.get('provider')}")
    print(f"ë¹„ìš©: ${response.metadata.get('cost', 0.0):.6f}")

asyncio.run(main())
```

### RDB ë°ì´í„° ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©

```python
import asyncio
from src.extraction import (
    RDBExtractorFactory, ExtractionConfig, ExtractionMode, DataFormat
)
from src.core.config import DatabaseConfig
from src.database.drivers import DatabaseType

async def main():
    # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
    database_config = DatabaseConfig(
        host="localhost",
        port=5432,
        database="mydb",
        user="postgres",
        password="password",
        database_type=DatabaseType.POSTGRESQL
    )
    
    # ì¶”ì¶œ ì„¤ì •
    extraction_config = ExtractionConfig(
        database_config=database_config,
        mode=ExtractionMode.INCREMENTAL,
        batch_size=1000,
        max_rows=10000,
        include_tables=["users", "orders", "products"],
        incremental_column="updated_at",
        validate_data=True,
        output_format=DataFormat.DICT
    )
    
    # ì¶”ì¶œê¸° ìƒì„±
    extractor = RDBExtractorFactory.create(extraction_config)
    
    try:
        # ëª¨ë“  í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        results = extractor.extract_all_tables()
        
        for result in results:
            if result.is_successful():
                print(f"í…Œì´ë¸” {result.metadata.name}ì—ì„œ {len(result.data)} í–‰ ì¶”ì¶œ ì™„ë£Œ")
                print(f"ì¶”ì¶œ ID: {result.extraction_id}")
                print(f"ì²˜ë¦¬ ì‹œê°„: {result.stats.total_time:.2f}ì´ˆ")
            else:
                print(f"ì¶”ì¶œ ì‹¤íŒ¨: {result.errors}")
        
        # ì ì§„ì  ë™ê¸°í™” ìƒíƒœ í™•ì¸
        summary = extractor.get_extraction_summary()
        print(f"ì „ì²´ ì¶”ì¶œ í†µê³„: {summary}")
        
    finally:
        extractor.close()

asyncio.run(main())
```

### íŒŒì¼ ì‹œìŠ¤í…œ ë°°ì¹˜ ì²˜ë¦¬ ì‚¬ìš©

```python
import asyncio
from pathlib import Path
from src.file_system import (
    FileProcessor, BatchProcessingConfig, ProcessingStrategy,
    RetryStrategy, ErrorSeverity
)

async def progress_callback(stats):
    """ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜"""
    print(f"ì§„í–‰ë¥ : {stats.progress_percentage:.1f}% "
          f"({stats.processed_items}/{stats.total_items})")
    print(f"ì²˜ë¦¬ ì†ë„: {stats.items_per_second:.1f} files/sec")
    print(f"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {stats.eta_seconds:.0f}ì´ˆ")

async def main():
    # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
    batch_config = BatchProcessingConfig(
        max_workers=8,
        batch_size=50,
        processing_strategy=ProcessingStrategy.PRIORITY_BASED,
        retry_strategy=RetryStrategy.EXPONENTIAL_BACKOFF,
        max_retries=3,
        memory_limit_mb=2048,
        enable_adaptive_batching=True,
        enable_eta_calculation=True,
        enable_gc=True
    )
    
    # íŒŒì¼ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    processor = FileProcessor(
        batch_config=batch_config,
        enable_change_detection=True,
        enable_metadata_extraction=True
    )
    
    try:
        # ë””ë ‰í† ë¦¬ ì²˜ë¦¬
        results = processor.process_directory(
            directory="/path/to/documents",
            file_patterns=["*.pdf", "*.docx", "*.txt", "*.md"],
            exclude_patterns=["*.tmp", "*.log"],
            max_depth=5,
            only_changed_files=True,
            progress_callback=progress_callback
        )
        
        # ê²°ê³¼ ì¶œë ¥
        summary = results["batch_processing"]["summary"]
        print(f"ì²˜ë¦¬ ì™„ë£Œ: {summary['successful']}ê°œ ì„±ê³µ, {summary['failed']}ê°œ ì‹¤íŒ¨")
        print(f"ì„±ê³µë¥ : {summary['success_rate']:.1%}")
        print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {summary['processing_time']:.2f}ì´ˆ")
        
        # ì—ëŸ¬ ë¶„ì„
        if results["batch_processing"]["errors"]:
            print("\nì˜¤ë¥˜ ë°œìƒ íŒŒì¼:")
            for error in results["batch_processing"]["errors"]:
                print(f"- {error['path']}: {error['error']}")
        
        # ì§„í–‰ë¥  ë° ì„±ëŠ¥ í†µê³„
        performance = results["batch_processing"]["performance"]
        print(f"\nì„±ëŠ¥ í†µê³„:")
        print(f"- í‰ê·  ì²˜ë¦¬ ì†ë„: {performance['average_throughput']:.1f} files/sec")
        print(f"- í”¼í¬ ì²˜ë¦¬ ì†ë„: {performance['peak_throughput']:.1f} files/sec")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory = results["batch_processing"]["memory"]
        print(f"- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory['current_mb']:.1f}MB")
        print(f"- ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory['peak_mb']:.1f}MB")
        
    except Exception as e:
        print(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    finally:
        processor.stop_processing()

asyncio.run(main())
```

### ì‹¤ì‹œê°„ ë°°ì¹˜ ì²˜ë¦¬ ì œì–´

```python
from src.file_system import FileProcessor, BatchProcessingConfig

# íŒŒì¼ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
processor = FileProcessor()

# ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œì‘
processing_task = asyncio.create_task(
    processor.process_directory("/large/dataset")
)

# ì²˜ë¦¬ ì¤‘ ì œì–´
await asyncio.sleep(5)
processor.pause_processing()  # ì¼ì‹œ ì •ì§€
print("ì²˜ë¦¬ ì¼ì‹œ ì •ì§€ë¨")

await asyncio.sleep(2)
processor.resume_processing()  # ì¬ê°œ
print("ì²˜ë¦¬ ì¬ê°œë¨")

# ìƒíƒœ ëª¨ë‹ˆí„°ë§
status = processor.get_processing_status()
print(f"í˜„ì¬ ìƒíƒœ: {status['state']}")
print(f"ì§„í–‰ë¥ : {status['progress']['progress_percentage']:.1f}%")

# ì²˜ë¦¬ ì™„ë£Œ ëŒ€ê¸°
results = await processing_task
```

### ê°œë³„ í…Œì´ë¸” ì¶”ì¶œ

```python
from src.extraction import GenericRDBExtractor, ExtractionConfig

# íŠ¹ì • í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¶”ì¶œ
extractor = GenericRDBExtractor(extraction_config)

# í•„í„°ë§ê³¼ í•¨ê»˜ ì¶”ì¶œ
result = extractor.extract_table_data(
    table_name="users",
    filters={"status": "active", "created_at": "> '2024-01-01'"},
    order_by="created_at DESC"
)

print(f"ì¶”ì¶œëœ í™œì„± ì‚¬ìš©ì ìˆ˜: {len(result.data)}")
print(f"í…Œì´ë¸” ë©”íƒ€ë°ì´í„°: {result.metadata.to_dict()}")
```

### ë²¡í„° íŒŒì´í”„ë¼ì¸ ì‚¬ìš©

```python
import asyncio
from src.pipeline import VectorPipeline, PipelineConfig
from src.embedding import EmbeddingManager, EmbeddingConfig, EmbeddingProvider
from src.milvus import MilvusClient
from src.text_processing import TextCleaner, TextSplitter

async def main():
    # íŒŒì´í”„ë¼ì¸ ì„¤ì •
    pipeline_config = PipelineConfig(
        batch_size=100,
        enable_parallel_processing=True,
        max_workers=8,
        enable_performance_monitoring=True,
        enable_error_recovery=True
    )
    
    # ì„ë² ë”© ì„¤ì •
    embedding_config = EmbeddingConfig(
        provider=EmbeddingProvider.OPENAI,
        model="text-embedding-3-large",
        api_key="your-openai-key",
        dimensions=1536
    )
    
    embedding_manager = EmbeddingManager([embedding_config])
    milvus_client = MilvusClient()
    
    # ë²¡í„° íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = VectorPipeline(
        config=pipeline_config,
        text_cleaner=TextCleaner(),
        text_splitter=TextSplitter(),
        embedding_manager=embedding_manager,
        milvus_client=milvus_client
    )
    
    # ë¬¸ì„œ ì²˜ë¦¬
    documents = [
        {"id": "doc1", "content": "ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.", "metadata": {"source": "textbook"}},
        {"id": "doc2", "content": "ë”¥ëŸ¬ë‹ì€ ì‹ ê²½ë§ì„ í™œìš©í•œ í•™ìŠµ ë°©ë²•ì…ë‹ˆë‹¤.", "metadata": {"source": "paper"}}
    ]
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    result = await pipeline.process_documents(documents)
    
    print(f"ì²˜ë¦¬ëœ ë¬¸ì„œ ìˆ˜: {result.processed_count}")
    print(f"ìƒì„±ëœ ë²¡í„° ìˆ˜: {result.vector_count}")
    print(f"ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
    print(f"ì„±ê³µë¥ : {result.success_rate:.1%}")

asyncio.run(main())
```

### ê³ ê¸‰ ë°°ì¹˜ ì²˜ë¦¬ ë° ì„±ëŠ¥ ìµœì í™”

```python
from src.pipeline import (
    BatchProcessor, BatchConfig, BatchStrategy,
    PerformanceOptimizer, OptimizationConfig, OptimizationLevel
)

# ê³ ê¸‰ ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
batch_config = BatchConfig(
    strategy=BatchStrategy.ADAPTIVE,
    base_batch_size=50,
    max_batch_size=500,
    memory_threshold_mb=1024,
    cpu_threshold_percent=80,
    enable_dynamic_sizing=True,
    enable_parallel_processing=True
)

# ì„±ëŠ¥ ìµœì í™” ì„¤ì •
optimization_config = OptimizationConfig(
    level=OptimizationLevel.AGGRESSIVE,
    enable_memory_optimization=True,
    enable_cpu_optimization=True,
    enable_adaptive_concurrency=True,
    enable_smart_caching=True,
    max_memory_usage_percent=85,
    max_cpu_usage_percent=90
)

# ê³ ê¸‰ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
advanced_pipeline = VectorPipeline(
    config=pipeline_config,
    batch_processor=BatchProcessor(batch_config),
    performance_optimizer=PerformanceOptimizer(optimization_config),
    # ... ê¸°íƒ€ ì»´í¬ë„ŒíŠ¸
)

# ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
async def monitor_pipeline_performance():
    while pipeline.is_running():
        metrics = pipeline.get_performance_metrics()
        print(f"ì²˜ë¦¬ìœ¨: {metrics.throughput_per_second:.1f} docs/sec")
        print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {metrics.memory_usage_mb:.1f}MB")
        print(f"CPU ì‚¬ìš©ë¥ : {metrics.cpu_usage_percent:.1f}%")
        await asyncio.sleep(10)

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ê³¼ í•¨ê»˜ ì‹¤í–‰
processing_task = asyncio.create_task(pipeline.process_large_dataset(documents))
monitoring_task = asyncio.create_task(monitor_pipeline_performance())

await asyncio.gather(processing_task, monitoring_task)
```

### ë©”íƒ€ë°ì´í„° ê°•í™” ë° ì ‘ê·¼ ì œì–´

```python
from src.pipeline import MetadataEnricher, EnrichmentConfig, EnrichmentLevel
from src.access_control import AccessControlManager

# ë©”íƒ€ë°ì´í„° ê°•í™” ì„¤ì •
enrichment_config = EnrichmentConfig(
    enable_language_detection=True,
    enable_pii_detection=True,
    enable_entity_extraction=True,
    enable_security_classification=True,
    enable_compliance_tagging=True,
    auto_classify_sensitivity=True
)

metadata_enricher = MetadataEnricher(
    config=enrichment_config,
    access_control_manager=AccessControlManager()
)

# ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ê°•í™”
enriched_metadata = await metadata_enricher.enrich_metadata(
    content="ì´ ë¬¸ì„œëŠ” ê¸°ë°€ ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì—°ë½ì²˜: john@company.com",
    base_metadata={"source": "internal_doc", "department": "hr"},
    user_id="user123",
    enrichment_level=EnrichmentLevel.COMPREHENSIVE
)

print(f"PII ê°ì§€: {enriched_metadata['content_analysis']['pii_detected']}")
print(f"ë³´ì•ˆ ë¶„ë¥˜: {enriched_metadata['access_control']['security_classification']}")
print(f"ì»´í”Œë¼ì´ì–¸ìŠ¤ íƒœê·¸: {enriched_metadata['compliance']}")
```

## ğŸ–¥ï¸ CLI ì¸í„°í˜ì´ìŠ¤

í”„ë¡œë•ì…˜ ë ˆë”” Click ê¸°ë°˜ CLI ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ RAG ì„œë²„ì˜ ëª¨ë“  ì¸¡ë©´ì„ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬

```bash
# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (í…Œì´ë¸” ìƒì„±, ê¸°ë³¸ ì—­í• /ê¶Œí•œ, ê´€ë¦¬ì ê³„ì •)
uv run python -m src.cli.main database init

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
uv run python -m src.cli.main database test

# ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸
uv run python -m src.cli.main database status

# ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
uv run python -m src.cli.main database backup --output backup.sql

# ë°ì´í„°ë² ì´ìŠ¤ ë³µì›
uv run python -m src.cli.main database restore --input backup.sql
```

### ì‚¬ìš©ì ê´€ë¦¬

```bash
# ìƒˆ ì‚¬ìš©ì ìƒì„±
uv run python -m src.cli.main user create --username john --email john@example.com --role user

# ê´€ë¦¬ì ì‚¬ìš©ì ìƒì„±
uv run python -m src.cli.main user create --username admin --email admin@company.com --role admin

# ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ
uv run python -m src.cli.main user list

# íŠ¹ì • ì—­í•  ì‚¬ìš©ìë§Œ ì¡°íšŒ
uv run python -m src.cli.main user list --role admin

# í™œì„± ì‚¬ìš©ìë§Œ ì¡°íšŒ
uv run python -m src.cli.main user list --active

# JSON í˜•ì‹ìœ¼ë¡œ ì‚¬ìš©ì ëª©ë¡ ì¶œë ¥
uv run python -m src.cli.main user list --format json
```

### ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë° êµ¬ì„±

```bash
# ëª¨ë“  LLM í”„ë¡œë°”ì´ë” í…ŒìŠ¤íŠ¸
uv run python -m src.cli.main model test-llm

# íŠ¹ì • í”„ë¡œë°”ì´ë” í…ŒìŠ¤íŠ¸
uv run python -m src.cli.main model test-llm --provider openai

# ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ë¡œ í…ŒìŠ¤íŠ¸
uv run python -m src.cli.main model test-llm --prompt "ì–‘ì ì»´í“¨íŒ…ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"

# ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸
uv run python -m src.cli.main model test-embedding

# ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
uv run python -m src.cli.main model benchmark --iterations 20

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
uv run python -m src.cli.main model list-models

# ëª¨ë¸ ì„¤ì • ë³€ê²½
uv run python -m src.cli.main model set-model --llm-provider openai --llm-model gpt-4
```

### ë°ì´í„° ê´€ë¦¬

```bash
# ë””ë ‰í† ë¦¬ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
uv run python -m src.cli.main data ingest --path ./documents --recursive

# íŠ¹ì • íŒŒì¼ í˜•ì‹ë§Œ ì²˜ë¦¬
uv run python -m src.cli.main data ingest --path ./docs --file-types pdf,docx

# ë°ì´í„° ë™ê¸°í™”
uv run python -m src.cli.main data sync --source filesystem

# ë°ì´í„° ìƒíƒœ í™•ì¸
uv run python -m src.cli.main data status

# ë°ì´í„° ì •ë¦¬
uv run python -m src.cli.main data cleanup --orphaned
```

### ì„¤ì • ê´€ë¦¬

```bash
# .env íŒŒì¼ í…œí”Œë¦¿ ìƒì„±
cp .env.example .env

# í™˜ê²½ ë³€ìˆ˜ë¥¼ í†µí•œ ì„¤ì • ê´€ë¦¬
# .env íŒŒì¼ì„ ì§ì ‘ í¸ì§‘í•˜ì—¬ ì„¤ì • ë³€ê²½

# í˜„ì¬ ì„¤ì • í™•ì¸
uv run python -m src.cli.main config show

# ì„¤ì • ìœ íš¨ì„± ê²€ì¦
uv run python -m src.cli.main config validate
```

### CLI ê³ ê¸‰ ê¸°ëŠ¥

```bash
# ë””ë²„ê·¸ ëª¨ë“œë¡œ ì‹¤í–‰
uv run python -m src.cli.main --debug database status

# ìƒì„¸ ë¡œê·¸ì™€ í•¨ê»˜ ì‹¤í–‰
uv run python -m src.cli.main --verbose user list

# ì»¤ìŠ¤í…€ í™˜ê²½ íŒŒì¼ ì‚¬ìš©
uv run python -m src.cli.main --env-file custom.env database init

# ë„ì›€ë§ ë³´ê¸°
uv run python -m src.cli.main --help
uv run python -m src.cli.main database --help
uv run python -m src.cli.main user --help
```

### CLI íŠ¹ì§•

- **Rich ì½˜ì†” ì¶œë ¥**: ì»¬ëŸ¬í’€í•œ í…Œì´ë¸”, ì§„í–‰ í‘œì‹œì¤„, ìƒíƒœ í‘œì‹œ
- **ê¸€ë¡œë²Œ ì˜µì…˜**: `--debug`, `--verbose`, `--env-file` ì§€ì›
- **ì…ë ¥ ê²€ì¦**: ì•ˆì „í•œ ì‚¬ìš©ì ì…ë ¥ ë° í™•ì¸ í”„ë¡¬í”„íŠ¸
- **ì—ëŸ¬ ì²˜ë¦¬**: í¬ê´„ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€ ë° ë³µêµ¬ ì œì•ˆ
- **ì§„í–‰ ìƒíƒœ**: ì¥ì‹œê°„ ì‘ì—…ì— ëŒ€í•œ ì‹¤ì‹œê°„ ì§„í–‰ë¥  í‘œì‹œ
- **ë„ì›€ë§ ì‹œìŠ¤í…œ**: ëª¨ë“  ëª…ë ¹ì— ëŒ€í•œ ìƒì„¸í•œ ë„ì›€ë§

## ğŸ§ª ê°œë°œ

### ê°œë°œ í™˜ê²½ ì„¤ì •

```bash
# ê°œë°œ ì˜ì¡´ì„± ì„¤ì¹˜
uv sync --group dev

# ì½”ë“œ í¬ë§·íŒ…
uv run black src/
uv run isort src/

# ë¦°íŒ…
uv run flake8 src/
uv run mypy src/
```

### í…ŒìŠ¤íŒ…

```bash
# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
uv run pytest

# ì»¤ë²„ë¦¬ì§€ ë¦¬í¬íŠ¸
uv run pytest --cov=src

# íŠ¹ì • í…ŒìŠ¤íŠ¸ ì‹¤í–‰
uv run pytest tests/unit/test_database_base.py
uv run pytest tests/unit/test_milvus_client.py

# CLI ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
uv run python -m src.cli.main database test
uv run python -m src.cli.main model test-llm --provider openai
```

#### ğŸ§ª í…ŒìŠ¤íŠ¸ í˜„í™©

**ì™„ë£Œëœ í…ŒìŠ¤íŠ¸ (110/223 í…ŒìŠ¤íŠ¸ í†µê³¼):**

**âœ… Task 2 - ë°ì´í„°ë² ì´ìŠ¤ ë ˆì´ì–´ í…ŒìŠ¤íŠ¸:**
- `test_database_base.py`: 21/21 í†µê³¼ - ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ë° íŒ©í† ë¦¬ í…ŒìŠ¤íŠ¸
- `test_database_health.py`: 14/14 í†µê³¼ - í—¬ìŠ¤ ì²´í¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸  
- `test_database_engine.py`: 21/21 í†µê³¼ - ì—”ì§„ ë° êµ¬ì„± í…ŒìŠ¤íŠ¸
- ì´ **56ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼** (Task 2 í•µì‹¬ ê¸°ëŠ¥ 100% ì»¤ë²„ë¦¬ì§€)

**âœ… Task 3 - Milvus í†µí•© í…ŒìŠ¤íŠ¸:**
- `test_milvus_client.py`: 30/30 í†µê³¼ - í´ë¼ì´ì–¸íŠ¸ ë° ì—°ê²° í’€ í…ŒìŠ¤íŠ¸
- ì´ **30ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼** (ì½”ì–´ Milvus ê¸°ëŠ¥ 100% ì»¤ë²„ë¦¬ì§€)

**ğŸ”„ ì§„í–‰ ì¤‘ì¸ í…ŒìŠ¤íŠ¸:**
- ê³ ê¸‰ Milvus ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ (ìŠ¤í‚¤ë§ˆ, RBAC, ê²€ìƒ‰ ë“±)
- ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸

**í…ŒìŠ¤íŠ¸ í’ˆì§ˆ ë³´ì¥:**
- SQLAlchemy ëª¨í‚¹ íŒ¨í„´ ì •í™•ì„± ê²€ì¦
- Milvus API í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
- ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ ê²€ì¦
- ì—°ê²° ìƒíƒœ ë° í—¬ìŠ¤ ì²´í¬ ê²€ì¦

### ë°ì´í„°ë² ì´ìŠ¤ ê¸°ëŠ¥

êµ¬í˜„ëœ ë°ì´í„°ë² ì´ìŠ¤ ë ˆì´ì–´ëŠ” ë‹¤ìŒì„ ì œê³µí•©ë‹ˆë‹¤:

1. **ë‹¤ì¤‘ ë°ì´í„°ë² ì´ìŠ¤ ì§€ì›**: PostgreSQL, MySQL, MariaDB, Oracle, SQL Server
2. **ê³ ê¸‰ ì—°ê²° í’€ë§**: ëª¨ë‹ˆí„°ë§ ë° ì„±ëŠ¥ ì¶”ì  í¬í•¨
3. **í—¬ìŠ¤ ëª¨ë‹ˆí„°ë§**: ë‹¤ë‹¨ê³„ í—¬ìŠ¤ ì²´í¬ ì‹œìŠ¤í…œ
4. **ì—ëŸ¬ ì²˜ë¦¬**: ì§€ëŠ¥í˜• ì—ëŸ¬ ë¶„ë¥˜ ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
5. **ì„œí‚· ë¸Œë ˆì´ì»¤**: ë³µì›ë ¥ ìˆëŠ” ìš´ì˜ì„ ìœ„í•œ ë‚´ê²°í•¨ì„± íŒ¨í„´
6. **ìŠ¤í‚¤ë§ˆ ì¸í…”ë¦¬ì „ìŠ¤**: ìë™ ë°ì´í„°ë² ì´ìŠ¤ ì¸íŠ¸ë¡œìŠ¤í™ì…˜ ë° ë¶„ì„

## ğŸ³ ë°°í¬

### Docker ë°°í¬

```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t rag-server .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -p 8000:8000 rag-server
```

### í”„ë¡œë•ì…˜ ì„¤ì •

```bash
# í”„ë¡œë•ì…˜ í™˜ê²½ ì„¤ì •
export APP_ENV=production
export SECRET_KEY=your-production-secret-key

# í”„ë¡œë•ì…˜ ì„œë²„ ì‹¤í–‰
uv run rag-server
```

## ğŸ” ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ

1. **ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë¬¸ì œ**
   - ë°ì´í„°ë² ì´ìŠ¤ ì„œë²„ ìƒíƒœ í™•ì¸
   - ì—°ê²° ì„¤ì • ê²€ì¦
   - ì—°ê²° í’€ ì„¤ì • ê²€í† 

2. **LLM API ë¬¸ì œ**
   - API í‚¤ ì„¤ì • í™•ì¸
   - ì†ë„ ì œí•œ í™•ì¸
   - API ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

3. **Milvus ì—°ê²° ë¬¸ì œ**
   - Milvus ì„œë²„ ì‹¤í–‰ ìƒíƒœ í™•ì¸
   - ë²¡í„° ì»¬ë ‰ì…˜ ì„¤ì • í™•ì¸

### ëª¨ë‹ˆí„°ë§

```bash
# ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸ í™•ì¸
tail -f logs/app.log

# ì—ëŸ¬ ë¡œê·¸ í™•ì¸
grep ERROR logs/app.log

# ë°ì´í„°ë² ì´ìŠ¤ í—¬ìŠ¤ ì²´í¬
uv run rag-cli health database
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### Milvus ë²¡í„° ê²€ìƒ‰ ìµœì í™”

```python
# ì¸ë±ìŠ¤ íƒ€ì…ë³„ ìµœì í™” ì „ëµ
index_configs = {
    "small_dataset": {
        "index_type": "FLAT",
        "metric_type": "L2"
    },
    "medium_dataset": {
        "index_type": "IVF_FLAT", 
        "metric_type": "L2",
        "params": {"nlist": 1024}
    },
    "large_dataset": {
        "index_type": "HNSW",
        "metric_type": "L2", 
        "params": {"M": 16, "efConstruction": 200}
    }
}

# ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ìµœì í™”
search_params = {
    "IVF_FLAT": {"nprobe": 64},
    "HNSW": {"ef": 128},
    "IVF_PQ": {"nprobe": 32}
}
```

### RBAC ë° ë³´ì•ˆ ìµœì í™”

```python
# ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ìµœì í™”
user_context = {
    "user_id": "user123",
    "group_ids": ["analysts", "researchers"],
    "permissions": ["read", "write"],
    "cache_ttl": 300  # 5ë¶„ ìºì‹œ
}

# ë©”íƒ€ë°ì´í„° í•„í„°ë§ ìµœì í™”
access_filter = 'user_id == "user123" OR JSON_CONTAINS(group_ids, "analysts")'
```

### ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

```python
# ì—°ê²° í’€ ìµœì í™”
pool_config = {
    "pool_size": 20,
    "max_overflow": 10,
    "pool_timeout": 30,
    "pool_recycle": 3600
}
```

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. ë ˆí¬ì§€í† ë¦¬ë¥¼ í¬í¬í•˜ì„¸ìš”
2. í”¼ì²˜ ë¸Œëœì¹˜ë¥¼ ìƒì„±í•˜ì„¸ìš” (`git checkout -b feature/new-feature`)
3. ë³€ê²½ì‚¬í•­ì„ ì»¤ë°‹í•˜ì„¸ìš” (`git commit -am 'Add new feature'`)
4. ë¸Œëœì¹˜ì— í‘¸ì‹œí•˜ì„¸ìš” (`git push origin feature/new-feature`)
5. Pull Requestë¥¼ ìƒì„±í•˜ì„¸ìš”

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸ“ ì§€ì›

- **ë¬¸ì„œ**: [Documentation](docs/)
- **ì´ìŠˆ íŠ¸ë˜ì»¤**: [GitHub Issues](https://github.com/your-repo/issues)
- **ì—°ë½ì²˜**: team@ragserver.com

---

**ì°¸ê³ **: ì´ í”„ë¡œì íŠ¸ëŠ” í¬ê´„ì ì¸ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬, í—¬ìŠ¤ ëª¨ë‹ˆí„°ë§ ë° ì—ëŸ¬ ì²˜ë¦¬ ê¸°ëŠ¥ì„ ê°–ì¶˜ í”„ë¡œë•ì…˜ ë ˆë”” êµ¬í˜„ì…ë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ ë ˆì´ì–´ëŠ” ì™„ì „íˆ êµ¬í˜„ë˜ì–´ ì—”í„°í”„ë¼ì´ì¦ˆ ì‚¬ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ğŸŒ ë‹¤ë¥¸ ì–¸ì–´

- [English](README_EN.md)