#!/usr/bin/env python3
"""
ë™ì  ì²­í‚¹ ì‹œìŠ¤í…œ ê°„ë‹¨ í…ŒìŠ¤íŠ¸ - EmbeddingManager ì—†ì´
"""

import sys
sys.path.insert(0, "src")

from src.core.config import get_config
from src.database.base import DatabaseManager
from src.episode.processor import EpisodeEmbeddingProcessor, EpisodeProcessingConfig
from sqlalchemy import text

def test_chunking_logic_only():
    """ì²­í‚¹ ë¡œì§ë§Œ í…ŒìŠ¤íŠ¸."""
    print("ğŸ§ª ë™ì  ì²­í‚¹ ë¡œì§ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    try:
        # ì„¤ì • ë° ë§¤ë‹ˆì € ì´ˆê¸°í™” (embedding_manager ì—†ì´)
        config = get_config()
        db_manager = DatabaseManager(config.database)
        
        # í”„ë¡œì„¸ì„œ ì„¤ì • (embedding_manager=Noneë¡œ ì„¤ì •)
        processor_config = EpisodeProcessingConfig(
            enable_content_cleaning=True,
            enable_chunking=True
        )
        
        processor = EpisodeEmbeddingProcessor(
            database_manager=db_manager,
            embedding_manager=None,  # Noneìœ¼ë¡œ ì„¤ì •
            config=processor_config
        )
        
        print("1. ğŸ“ ëª¨ë¸ max_tokens í™•ì¸ (fallback í…ŒìŠ¤íŠ¸):")
        max_tokens = processor._get_model_max_tokens()
        print(f"   í´ë°± max_tokens: {max_tokens}")
        
        print("\n2. ğŸ¯ ìµœì  ì²­í¬ ì„¤ì • ê³„ì‚°:")
        chunk_size, overlap = processor._get_optimal_chunk_settings()
        print(f"   ê³„ì‚°ëœ ì²­í¬ í¬ê¸°: {chunk_size}ì")
        print(f"   ê³„ì‚°ëœ ê²¹ì¹¨: {overlap}ì")
        print(f"   ì²­í‚¹ ì„ê³„ê°’: {int(max_tokens * 0.85)}í† í° (â‰ˆ{int(max_tokens * 0.85 / 1.5)}ì)")
        
        print("\n3. ğŸ“Š ì‹¤ì œ ëª¨ë¸ ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸:")
        # ì‹¤ì œ jeffh/intfloat-multilingual-e5-large-instruct ëª¨ë¸ì˜ 512 í† í° ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ë™ ê³„ì‚°
        actual_max_tokens = 512
        actual_safe_tokens = int(actual_max_tokens * 0.85)  # 435
        actual_safe_chars = int(actual_safe_tokens / 1.5)   # 290
        actual_chunk_size = min(1500, actual_safe_chars)    # 290
        actual_overlap = max(20, min(200, int(actual_chunk_size * (200/1500))))  # 39
        
        print(f"   ì‹¤ì œ ëª¨ë¸ max_tokens: {actual_max_tokens}")
        print(f"   ì‹¤ì œ ê³„ì‚° ì²­í¬ í¬ê¸°: {actual_chunk_size}ì")
        print(f"   ì‹¤ì œ ê³„ì‚° ê²¹ì¹¨: {actual_overlap}ì")
        print(f"   ì‹¤ì œ ì²­í‚¹ ì„ê³„ê°’: {actual_safe_tokens}í† í° (â‰ˆ{int(actual_safe_tokens / 1.5)}ì)")
        
        print("\n4. ğŸ” ì‹¤ì œ ì—í”¼ì†Œë“œë¡œ ì²­í‚¹ í…ŒìŠ¤íŠ¸:")
        
        # í…ŒìŠ¤íŠ¸ìš© ì—í”¼ì†Œë“œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        with db_manager.get_connection() as conn:
            result = conn.execute(text("""
                SELECT episode_id, episode_title, content, LENGTH(content) as content_length
                FROM episode 
                WHERE LENGTH(content) > 100
                ORDER BY LENGTH(content) DESC
                LIMIT 5
            """))
            episodes = result.fetchall()
        
        for ep in episodes:
            content_length = ep.content_length
            estimated_tokens = int(content_length * 1.5)
            
            # ì‹¤ì œ ëª¨ë¸ ê¸°ì¤€ìœ¼ë¡œ ì²­í‚¹ íŒë‹¨
            should_chunk = estimated_tokens > actual_safe_tokens
            chunk_info = "ì²­í‚¹ í•„ìš”" if should_chunk else "ë‹¨ì¼ ì„ë² ë”©"
            
            print(f"   Episode {ep.episode_id}: {content_length}ì ({estimated_tokens}í† í°) â†’ {chunk_info}")
            
            if should_chunk:
                # ì‹¤ì œ ì²­í‚¹ í…ŒìŠ¤íŠ¸
                chunks = processor._split_content_into_chunks(ep.content, actual_chunk_size, actual_overlap)
                total_chunk_chars = sum(len(chunk) for chunk in chunks)
                print(f"     â””â”€ {len(chunks)}ê°œ ì²­í¬ ìƒì„±, ì´ {total_chunk_chars}ì (ì›ë³¸: {content_length}ì)")
                
                # ê° ì²­í¬ê°€ í† í° ì œí•œ ë‚´ì¸ì§€ í™•ì¸
                for i, chunk in enumerate(chunks[:3]):  # ì²˜ìŒ 3ê°œë§Œ í™•ì¸
                    chunk_tokens = int(len(chunk) * 1.5)
                    safe = "âœ…" if chunk_tokens <= actual_max_tokens else "âŒ"
                    print(f"        ì²­í¬ {i+1}: {len(chunk)}ì ({chunk_tokens}í† í°) {safe}")
        
        print(f"\nğŸ¯ ì²­í‚¹ ì „í›„ ë¹„êµ:")
        print(f"   ê¸°ì¡´ ì„¤ì • (1500ì ì²­í¬, 2000í† í° ì„ê³„ê°’):")
        print(f"   â€¢ 1333ì ì´ìƒ ì—í”¼ì†Œë“œ â†’ 1500ì ì²­í¬ (2250í† í°, 77% truncated)")
        print(f"   ìƒˆ ì„¤ì • ({actual_chunk_size}ì ì²­í¬, {actual_safe_tokens}í† í° ì„ê³„ê°’):")
        print(f"   â€¢ {int(actual_safe_tokens/1.5)}ì ì´ìƒ ì—í”¼ì†Œë“œ â†’ {actual_chunk_size}ì ì²­í¬ ({int(actual_chunk_size*1.5)}í† í°, ì™„ì „ ë³´ì¡´)")
        
        print(f"\nğŸ‰ ë™ì  ì²­í‚¹ ë¡œì§ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        if 'db_manager' in locals():
            db_manager.close()

if __name__ == "__main__":
    success = test_chunking_logic_only()
    print(f"\n{'='*60}")
    if success:
        print("ğŸ‰ ë™ì  ì²­í‚¹ ë¡œì§ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!")
        print("ì‹¤ì œ ëª¨ë¸ì—ì„œëŠ” 290ì ì²­í¬ë¡œ 512í† í° ì œí•œ ë‚´ì—ì„œ ì™„ë²½í•˜ê²Œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
    else:
        print("âš ï¸ ë™ì  ì²­í‚¹ ë¡œì§ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
    sys.exit(0 if success else 1)