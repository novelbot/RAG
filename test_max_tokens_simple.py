#!/usr/bin/env python3
"""
OpenAI ì„ë² ë”© ëª¨ë¸ì˜ max_tokens ì„¤ì • í™•ì¸ (ê°„ë‹¨ ë²„ì „)
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

def test_max_tokens_config():
    """MODEL_MAX_TOKENS ì„¤ì • í™•ì¸"""
    
    print("=" * 80)
    print("OpenAI ì„ë² ë”© ëª¨ë¸ max_tokens ì„¤ì • í™•ì¸")
    print("=" * 80)
    
    # Import and check MODEL_MAX_TOKENS
    from src.embedding.langchain_embeddings import LangChainEmbeddingProvider
    
    print("\nğŸ“Š MODEL_MAX_TOKENS ë”•ì…”ë„ˆë¦¬ ë‚´ìš©:")
    print("-" * 40)
    
    # OpenAI models
    openai_models = [
        "text-embedding-ada-002",
        "text-embedding-3-small",
        "text-embedding-3-large"
    ]
    
    for model in openai_models:
        max_tokens = LangChainEmbeddingProvider.MODEL_MAX_TOKENS.get(model, "NOT FOUND")
        print(f"  {model}: {max_tokens} tokens")
    
    print("\nğŸ“Š ì²­í‚¹ ì„ê³„ê°’ ê³„ì‚°:")
    print("-" * 40)
    
    for model in openai_models:
        max_tokens = LangChainEmbeddingProvider.MODEL_MAX_TOKENS.get(model, 2048)
        threshold = int(max_tokens * 0.85)
        char_limit = int(threshold / 1.5)
        
        print(f"\n{model}:")
        print(f"  â€¢ Max Tokens: {max_tokens}")
        print(f"  â€¢ ì²­í‚¹ ì„ê³„ê°’ (85%): {threshold} tokens")
        print(f"  â€¢ í•œêµ­ì–´ ë¬¸ì ì œí•œ: ~{char_limit:,} ê¸€ì")
        print(f"  â€¢ ì²˜ë¦¬ ë°©ì‹:")
        print(f"    - {char_limit:,}ì ì´í•˜ â†’ ë‹¨ì¼ ì„ë² ë”© ìƒì„±")
        print(f"    - {char_limit:,}ì ì´ˆê³¼ â†’ ìë™ ì²­í‚¹ í›„ ê°œë³„ ì„ë² ë”©")
    
    print("\nâœ… OpenAI ëª¨ë¸ë“¤ì˜ max_tokensê°€ ëª¨ë‘ 8191ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    print("âœ… Ollama ëª¨ë¸(~1,160ì)ë³´ë‹¤ ì•½ 4ë°° ë§ì€ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

def compare_with_ollama():
    """Ollama vs OpenAI ë¹„êµ"""
    
    print("\n" + "=" * 80)
    print("Ollama vs OpenAI ì„ë² ë”© ëª¨ë¸ ë¹„êµ")
    print("=" * 80)
    
    comparisons = [
        ("Ollama (jeffh/intfloat)", 2048, "ê¸°ë³¸ ì„¤ì •"),
        ("OpenAI (text-embedding-ada-002)", 8191, "ìƒˆë¡œ ì¶”ê°€"),
        ("OpenAI (text-embedding-3-small)", 8191, "ìƒˆë¡œ ì¶”ê°€"),
        ("OpenAI (text-embedding-3-large)", 8191, "ìƒˆë¡œ ì¶”ê°€"),
    ]
    
    print("\n| ëª¨ë¸ | Max Tokens | í•œêµ­ì–´ ë¬¸ì ì œí•œ | ìƒíƒœ |")
    print("|------|------------|-----------------|------|")
    
    for model, max_tokens, status in comparisons:
        char_limit = int(max_tokens * 0.85 / 1.5)
        print(f"| {model:<30} | {max_tokens:>10,} | {char_limit:>15,}ì | {status} |")
    
    print("\nğŸ“Œ ê²°ë¡ :")
    print("  â€¢ OpenAI ëª¨ë¸ ì‚¬ìš© ì‹œ ëŒ€ë¶€ë¶„ì˜ ì—í”¼ì†Œë“œëŠ” ì²­í‚¹ ì—†ì´ ì²˜ë¦¬ ê°€ëŠ¥")
    print("  â€¢ 4,600ì ì´ìƒì˜ ê¸´ í…ìŠ¤íŠ¸ë§Œ ìë™ ì²­í‚¹ ì²˜ë¦¬")
    print("  â€¢ OllamaëŠ” 1,160ì ì´ìƒë¶€í„° ì²­í‚¹ í•„ìš”")

if __name__ == "__main__":
    test_max_tokens_config()
    compare_with_ollama()
    
    print("\n" + "=" * 80)
    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 80)